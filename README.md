# ðŸ“– MiniRAG-Gemini

MiniRAG-Gemini is a lightweight Retrieval-Augmented Generation (RAG) demo built with:

- *Streamlit* (frontend UI)  
- *Qdrant* (vector database)  
- *Cohere* (embeddings)  
- *Gemini (Google Generative AI)* (LLM for answering)  

It allows you to upload or paste documents, index them into Qdrant, and then ask natural language questions. The system retrieves the most relevant chunks and feeds them into Gemini for contextual answers.

---

## ðŸš€ Features
- Upload .txt, .md, or .pdf files  
- Automatic text chunking with overlap for better retrieval  
- Embedding via *Cohere*  
- Vector storage & retrieval via *Qdrant*  
- Answer generation via *Gemini* (google.generativeai)  
- Streaming answers in the UI  
- Sources and snippets displayed for transparency  

---

## ðŸ“‚ Project Structure
```bash
mini_rag_app/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                # Streamlit entrypoint
â”‚   â”œâ”€â”€ embeddings.py         # Extract, chunk, and embed text
â”‚   â”œâ”€â”€ vectorstore.py        # Qdrant operations
â”‚   â”œâ”€â”€ retriever.py          # Retrieve top-k documents
â”‚   â”œâ”€â”€ llm_answer.py         # Prompt + Gemini answer generator
â”‚   â”œâ”€â”€ config.py             # Configuration constants
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md